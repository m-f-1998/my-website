name: Weekly Repo Backup

on:
  schedule:
    - cron: "0 0 * * 0" # Every Sunday at 00:00 UTC
  workflow_dispatch: # allow manual runs

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # full history (includes .git)

      - name: Create backup archive
        run: |
          # derive repo name (strip owner/)
          REPO_FULL="${{ github.repository }}"
          REPO_NAME="${REPO_FULL#*/}"
          DATE_STR=$(date -u +'%Y-%m-%d_%H-%M-%SZ')
          FILENAME="${REPO_NAME}_${DATE_STR}.tar.gz"

          echo "Creating archive $FILENAME"
          tar -czf "$FILENAME" .

          # expose to later steps
          echo "ARCHIVE=$FILENAME" >> $GITHUB_ENV
          echo "REPO_FOLDER=$REPO_NAME" >> $GITHUB_ENV

      - name: Install AWS CLI
        run: |
          python -m pip install --upgrade pip
          pip install awscli --upgrade --user
          export PATH="$HOME/.local/bin:$PATH"
          aws --version

      - name: Upload to Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: auto
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          BUCKET: github-backups
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          echo "Uploading $ARCHIVE -> s3://${BUCKET}/${REPO_FOLDER}/"
          aws --endpoint-url "$ENDPOINT" s3 cp "$ARCHIVE" "s3://${BUCKET}/${REPO_FOLDER}/${ARCHIVE}"
          echo "Upload finished."

      - name: Cleanup local archive
        if: always()
        run: |
          rm -f "$ARCHIVE" || true
